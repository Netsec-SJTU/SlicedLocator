# @Time: 2022.5.28 15:05
# @Author: Bolun Wu
# We propose a code representation format called sliced dependency graph (SDG).
# SDG conbines control dependence, data dependence and call dependence.
# it is generated by forward and backward slicing based on code of interest (COI).

import json
import multiprocessing
import os
import random

import tqdm

from joern import check_node_edge_file, generate_pdgc, root_dir, run_joern
from normalizer import comment_remover, symbolize_snippet, tokenize_code

# constants
seed = 42
coi_dir = os.path.join(root_dir, 'coi') # code of interest
with open(os.path.join(coi_dir, 'sensi_api.txt'), 'r') as f:
    sensi_apis = f.readlines()
    sensi_apis = list(map(lambda x: x.strip(), sensi_apis))

with open(os.path.join(coi_dir, 'joern_math_pat.txt'), 'r') as f:
    math_pattern = f.readlines()
    math_pattern = list(map(lambda x: x.strip(), math_pattern))

etype_onehot = {
    'CDG': 0,
    'REACHING_DEF': 1,
    'CALL': 2
}

def extract_coi_func(source_code, nodes):
    coi_lines = []
    for _, row in nodes.iterrows():
        line_num = int(row['lineNumber'])
        if line_num in coi_lines: continue
        
        if row['_label'] == 'CALL' and not row['name'].startswith('<operator'):
            if ('execlp' in row['code'] and 'EXECLP(' in source_code[line_num-1]) or \
               ('execl(' in row['code'] and 'EXECL(' in source_code[line_num-1]):
                source_code[line_num-1] = row['code']
            line_code = source_code[line_num-1]
            
            for api in sensi_apis:
                if api in line_code:
                    coi_lines.append(line_num)
                    break
    return coi_lines


def extract_coi_ptr(source_code, nodes):
    coi_lines = []
    for _, row in nodes.iterrows():
        line_num = int(row['lineNumber'])
        if line_num in coi_lines: continue
        
        if row['_label'] == 'METHOD_PARAMETER_IN' or row['_label'] == 'LOCAL':
            line_code = source_code[line_num-1]
            # consider int * a = 10 * 20;
            # we must extract code before `=` first
            if '=' in line_code:
                line_code = line_code.split('=')[0]
            if '*' in line_code:
                coi_lines.append(line_num)
                
    return coi_lines
    

def extract_coi_array(source_code, nodes):
    coi_lines = []
    for _, row in nodes.iterrows():
        line_num = int(row['lineNumber'])
        if line_num in coi_lines: continue
        
        if row['_label'] == 'METHOD_PARAMETER_IN' or row['_label'] == 'LOCAL':
            line_code = source_code[line_num-1]
            # we must extract code before `=` first
            if '=' in line_code:
                line_code = line_code.split('=')[0]
            if '[' in line_code:
                coi_lines.append(line_num)
        elif row['_label'] == 'CALL' and row['name'] == '<operator>.indirectIndexAccess':
            coi_lines.append(line_num)
                
    return coi_lines


def extract_coi_math(source_code, nodes):
    coi_lines = []
    for _, row in nodes.iterrows():
        line_num = int(row['lineNumber'])
        if line_num in coi_lines: continue
        
        if row['name'] in math_pattern:
            coi_lines.append(line_num)
                
    return coi_lines


def extract_coi_jump(source_code, nodes):
    coi_lines = []
    for _, row in nodes.iterrows():
        line_num = int(row['lineNumber'])
        if line_num in coi_lines: continue
        
        if row['_label'] == 'CONTROL_STRUCTURE':
            coi_lines.append(line_num)
            
    return coi_lines
    

# generate sliced dependency graph (SDG)
def generate_sdg(filepath, save_csv=False):
    if not check_node_edge_file(filepath):
        return []

    pdgc, nodes = generate_pdgc(filepath, save_csv=save_csv)

    with open(filepath, 'r', errors='ignore') as f:
        source_code = f.readlines()
        source_code = list(map(lambda x: x.strip(), source_code))
    
    ## * remove comment in source_code here !!
    ## * it's important because comment may disturb coi extraction
    source_code = list(map(comment_remover, source_code))
    source_code = list(map(lambda x: x.strip(), source_code))
    
    node_lines = []
    for n in pdgc.nodes: node_lines.append(n)
    node_lines.sort()
    
    def __filter_node(row):
        if int(row['lineNumber']) not in node_lines:
            return False
        return True
    m = nodes.apply(__filter_node, axis=1)
    nodes = nodes[m]

    coi_func_lines = extract_coi_func(source_code, nodes)
    coi_ptr_lines = extract_coi_ptr(source_code, nodes)
    coi_array_lines = extract_coi_array(source_code, nodes)
    coi_math_lines = extract_coi_math(source_code, nodes)

    ## * remove repeated coi lines
    unique_coi_lines = set()
    coi_func_lines = list(filter(lambda x: x not in unique_coi_lines, coi_func_lines))
    unique_coi_lines = set(list(unique_coi_lines)+coi_func_lines)
    coi_ptr_lines = list(filter(lambda x: x not in unique_coi_lines, coi_ptr_lines))
    unique_coi_lines = set(list(unique_coi_lines)+coi_ptr_lines)
    coi_array_lines = list(filter(lambda x: x not in unique_coi_lines, coi_array_lines))
    unique_coi_lines = set(list(unique_coi_lines)+coi_array_lines)
    coi_math_lines = list(filter(lambda x: x not in unique_coi_lines, coi_math_lines))
    unique_coi_lines = set(list(unique_coi_lines)+coi_math_lines)
    
    coi_lines = {
        'call': coi_func_lines,
        'ptr': coi_ptr_lines,
        'array': coi_array_lines,
        'math': coi_math_lines,
    }

    ## * tokenize source code
    source_code = list(map(lambda c: ' '.join(tokenize_code(c)), source_code))
    
    ## * extract SDGs based on lines of coi (code of interest)
    sdgs = []
    for coi_type, lines in coi_lines.items():
        for line in lines:
            sliced_lines = set()
            
            ## backward slicing
            queue, visited = [line], set([line])
            while queue:
                current = queue.pop(0)
                sliced_lines.add(current)
                for pred in pdgc.predecessors(current):
                    if pred not in visited:
                        visited.add(pred)
                        queue.append(pred)
            
            ## forward slicing
            queue, visited = [line], set([line])
            while queue:
                current = queue.pop(0)
                sliced_lines.add(current)
                for succ in pdgc.successors(current):
                    if succ not in visited:
                        visited.add(succ)
                        queue.append(succ)
            
            if len(sliced_lines) < 3: continue
            
            sdg = pdgc.subgraph(list(sliced_lines)).copy()
            
            nline_to_nid = {}
            node_line, node_line_no, edge, edge_type = [], [], [], []
            
            for n in sdg.nodes: node_line_no.append(n)
            node_line_no.sort()
            for nline in node_line_no:
                nline_to_nid[nline] = len(node_line)
                node_line.append(source_code[nline-1])
            
            ## * symbolize + tokenize for SDG snippet
            node_line_sym = symbolize_snippet(node_line)
            node_line_sym = list(map(lambda c: ' '.join(tokenize_code(c)), node_line_sym))
            
            ## * dealing with TYPE containing "CWE" in SARD dataset
            ## e.g. CWE78_OS_Command_Injection__char_environment_w32spawnl_67_structType myStruct
            def __map_CWEType(line_sym):
                splitted = line_sym.split(' ')
                for i, word in enumerate(splitted):
                    if word == 'badStruct':
                        splitted[i] = 'my_struct'
                    if 'CWE' in word and 'Type' in word:
                        found = False
                        for _type in ['wchar_t', 'char', 'int64_t', 'int', 'long', 'short', 'float']:
                            if _type in word:
                                splitted[i] = _type
                                found = True
                                break
                        if not found:
                            splitted[i] = 'UnknownType'
                return ' '.join(splitted)
            node_line_sym = list(map(lambda c: __map_CWEType(c), node_line_sym))
            
            hash_edges = []
            for e in sdg.edges:
                src_line, dst_line = e[0], e[1]
                etype = sdg.edges[e]['etype']
                etype = etype_onehot[etype]
                edge.append([nline_to_nid[src_line], nline_to_nid[dst_line]])
                hash_edges.append(f'{nline_to_nid[src_line]}_{nline_to_nid[dst_line]}')
                edge_type.append(etype)

            ## * calculate hash for symbolized code lines joined with edges
            hash_content = '\n'.join(node_line_sym)
            hash_content = hash_content + '@' + str(sorted(hash_edges))
            sym_hash = hash(hash_content)
            
            info = {
                'filepath': filepath,
                'coi_line': line,
                'coi_type': coi_type,
                'node_line': node_line,
                'node_line_sym': node_line_sym,
                'node_line_no': node_line_no,
                'edge': edge,
                'edge_type': edge_type,
                'sym_hash': sym_hash
            }
            
            sdgs.append(info)

    ## ! duplicates cannot be removed here
    ## ! because we need to preserve those black sdgs

    return sdgs


def single_file_sdg_worker(merge_args):
    filepath, label_info = merge_args
    coarse_label = label_info['gt_coarse']
    sdgs = generate_sdg(filepath)
    
    # attach label information
    for sdg in sdgs:
        sdg['vul_line_no'] = []
        sdg['label'] = 0
        if coarse_label == 1: # vulnerable sample
            fine_label = label_info['gt_fine']
            for sdg_line in sdg['node_line_no']:
                if sdg_line in fine_label:
                    sdg['label'] = 1
                    sdg['vul_line_no'].append(sdg_line)
    return sdgs
        

def generate_sdg_for_dataset(label_path, dataset_name, num_proc=8):
    assert num_proc >= 1
    assert os.path.exists(label_path)
    
    label_dir = os.path.dirname(label_path)
    save_dir = os.path.join(label_dir, 'sdg')
    os.makedirs(save_dir, exist_ok=True)
    
    with open(label_path, 'r') as f: fp_to_label = json.load(f)
    
    print(f'Extracting SDGs for all {dataset_name} samples...')
    all_sdgs, filepath_list = [], list(fp_to_label.keys())
    # * single process
    if num_proc == 1:
        for filepath in filepath_list:
            label_info = fp_to_label[filepath]
            sdgs = single_file_sdg_worker((filepath, label_info))
            all_sdgs += sdgs
    # * multiple processes
    elif num_proc > 1:
        merge_args = [(fp, fp_to_label[fp]) for fp in filepath_list]
        pool = multiprocessing.Pool(num_proc)
        res = list(tqdm.tqdm(pool.imap(single_file_sdg_worker, merge_args), total=len(merge_args)))
        for group in res: all_sdgs += group
    
    print('Saving raw SDGs (with duplicates)...')
    with open(os.path.join(save_dir, f'{dataset_name}_sdg.json'), 'w') as f:
        json.dump(all_sdgs, f, indent=1)
    print(f'Total raw SDGs: {len(all_sdgs)}.')
    
    # * remove duplicates
    print('Cleaning SDGs (removing duplicates)...')
    hash_to_sdg = dict()
    for sdg in all_sdgs:
        sdg_hash = sdg['sym_hash']
        if sdg_hash not in hash_to_sdg:
            hash_to_sdg[sdg_hash] = sdg
        else:
            sdg_in_dict = hash_to_sdg[sdg_hash]
            # keep the vulnerable one
            if sdg_in_dict['label'] == 1 and sdg['label'] == 0:
                continue
            elif sdg_in_dict['label'] == 0 and sdg['label'] == 1:
                hash_to_sdg[sdg_hash] = sdg
            # if both white / both black, randomly choose one
            else:
                hash_to_sdg[sdg_hash] = sdg if random.random() > 0.5 else sdg_in_dict
    all_sdgs = list(hash_to_sdg.values())
    
    print('Saving cleaned SDGs (without duplicates)...')
    with open(os.path.join(save_dir, f'{dataset_name}_sdg_clean.json'), 'w') as f:
        json.dump(all_sdgs, f, indent=1)
    print(f'Total cleaned SDGs: {len(all_sdgs)}.')
    
    return all_sdgs


if __name__ == '__main__':
    fp = 'test/scpy2-bad.c'
    fp_name = os.path.basename(fp)
    fp_dir = os.path.dirname(fp)
    run_joern(fp)
    sdgs = generate_sdg(fp, save_csv=False)
    with open(os.path.join(fp_dir, f'{fp_name}_sdg.json'), 'w') as f:
        json.dump(sdgs, f, indent=1)
    print(len(sdgs))
